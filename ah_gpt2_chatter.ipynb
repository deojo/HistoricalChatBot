{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuUm5zLsOl1t",
        "outputId": "2cc87782-a6bd-4be1-d248-cdbfd005d0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "<gpt2>: Yes, Indian food spicy is spicy. It's a mixture of different types of food, including pungency, onions, and turmeric. It's spicy enough to make a pungency stick in the Indian diet. <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<gpt2>: \"Meat Processing: A History\" by Helen Hardcase is a great resource for understanding the meat processing industry in ancient Rome. It's a comprehensive database of meat products and ingredients, including ingredients, and provides a comprehensive database of meat products and ingredients. It's a great resource for understanding the meat processing industry in ancient Rome and its early days, including\n",
            "<gpt2>: \"Yes, it does. The process of butchering the meat was a lengthy and expensive process, and it involved a lot of effort to remove the poisonous parts. The process involved collecting the meat, collecting the poisonous parts, and then refining the mixture to make it more palaty. Once the mixture was available, the process was done, the\n",
            "<user>: \"Have you read the \"Age of the French Revolution\" series by Claude Manceron?\"\n",
            "<gpt2>: \"Yes, I have. It's a collection of essays and articles written by historians and is a great resource for understanding the revolution. It's a great resource for understanding the social and cultural aspects of the revolution. <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> Yes,Yes,Yes, I have read the book\n",
            "<gpt2>: \"bye\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Thu Jun  4 10:02:47 2023\n",
        "@author: DSJoshi\n",
        "\"\"\"\n",
        "\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "def infer(inp):\n",
        "    inp = \"<startofstring> \"+inp+\" <bot>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"]\n",
        "    a = inp[\"attention_mask\"]\n",
        "    outputs = model.generate(X,\n",
        "                            attention_mask=a,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            max_new_tokens=70,\n",
        "                            no_repeat_ngram_size=30,\n",
        "                            do_sample=True,\n",
        "                            top_k=10,\n",
        "                            top_p=0.7,\n",
        "                            temperature = 0.001,\n",
        "                            num_return_sequences = 1)\n",
        "    bot_response = 'I am not able to answer this.'\n",
        "    outputs = tokenizer.decode(outputs[0])\n",
        "    outputs = outputs.split('<bot>:')\n",
        "    if len(outputs) == 2:\n",
        "      bot_response = outputs[1].strip()\n",
        "      bot_response = bot_response.replace('<pad>', '')\n",
        "    return bot_response\n",
        "\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('Deojoandco/ah-GPT2-v4')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('Deojoandco/ah-GPT2-v4')\n",
        "\n",
        "for i in range(5):\n",
        "    query = input('<user>: ')\n",
        "    if query.lower() == 'q':\n",
        "      break\n",
        "    output = infer(query)\n",
        "    print(f'<gpt2>: {output}')\n",
        "\n",
        "print('<gpt2>: bye')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}